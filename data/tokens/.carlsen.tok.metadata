{"timestamp": 1716135230.252126, "stored_source_code": "# declare a list tasks whose products you want to use as inputs\nupstream = None\nupstream = [\"checkDuplicates\"]\ndef tokenize_data(xLAN_path, token_path, tokenized_path):\n    from src.tokenizer.tokenizer import tokenize_file\n\n    tokenize_file(\n        token_path, xLAN_path, tokenized_path, batch_size=20000\n    )  # eventually smaller batch size\ntokenize_data(xLAN_path, token_path, tokenized_path)", "params": {"xLAN_path": "./data/xlan/carlsen.xlanplus", "token_path": "./src/tokenizer/xlanplus_tokens.json", "tokenized_path": "./data/tokens/carlsen.tok"}}