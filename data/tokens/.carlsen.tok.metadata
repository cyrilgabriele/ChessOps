{"timestamp": 1715161419.108441, "stored_source_code": "# declare a list tasks whose products you want to use as inputs\nupstream = None\nupstream = [\"checkDuplicates\"]\ndef tokenize_data():\n    from src.tokenizer.tokenizer import tokenize_file\n\n    xLAN_path = \"./data/xlan/carlsen.xlanplus\"  # Inout Path\n    token_path = \"./src/tokenizer/xlanplus_tokens.json\"  # keep this, correct like this\n    tokenized_path = \"./data/tokens/carlsen.tok\"  # Output path\n\n    tokenize_file(\n        token_path, xLAN_path, tokenized_path, batch_size=20000\n    )  # eventually smaller batch size\ntokenize_data()", "params": {}}