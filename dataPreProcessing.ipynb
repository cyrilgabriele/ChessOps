{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert PGN to extended LAN (xLAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessing.pgn_to_xlan import pgn_to_xlan\n",
    "\n",
    "\n",
    "pgn_path = \"D:\\LEON Safe\\Datasets\\lichess_db_standard_rated_2014-09.pgn\"\n",
    "lan_path = \"D:\\LEON Safe\\Datasets\\lichess_db_standard_rated_2014-09.xlan\"\n",
    "\n",
    "min_number_of_moves_per_game = 0\n",
    "number_of_games_to_write = -1  # -1 for all games\n",
    "\n",
    "pgn_to_lan = pgn_to_xlan(\n",
    "    pgn_path,\n",
    "    lan_path,\n",
    "    min_number_of_moves_per_game=min_number_of_moves_per_game,\n",
    "    number_of_games_to_write=number_of_games_to_write,\n",
    "    generate_all_moves=False,\n",
    "    log=False,\n",
    "    xLanPlus=False,\n",
    ")\n",
    "\n",
    "pgn_to_lan.convert_pgn_parallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Common and Duplicate Lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use check_duplicates_and_common_lines to check if there are duplicates or common lines in two files.\n",
    "\"\"\"\n",
    "\n",
    "from src.data_preprocessing.check_duplicates_and_common_lines import (\n",
    "    check_duplicates_and_common_lines,\n",
    ")\n",
    "\n",
    "training_file = \"D:\\LEON Safe\\Datasets\\lichess_db_standard_rated_2023-09.xlanplus\"\n",
    "validation_file = (\n",
    "    \"D:\\LEON Safe\\Datasets\\lichess_db_standard_rated_2014-09_1000Lines.xlan\"\n",
    ")\n",
    "\n",
    "check_duplicates_and_common_lines(\n",
    "    training_file,\n",
    "    validation_file,\n",
    "    delete_common=False,\n",
    "    delete_dubplicates_from_file_1=True,\n",
    "    delete_dubplicates_from_file_2=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenizer.tokenizer import tokenize_file\n",
    "\n",
    "xLAN_path = \"D:\\LEON Safe\\Datasets\\lichess_db_standard_rated_2023-09.xlanplus\"\n",
    "token_path = \"./src/tokenizer/xlanplus_tokens.json\"\n",
    "tokenized_path = \"D:\\LEON Safe\\Datasets\\lichess_db_standard_rated_2023-09.tok\"\n",
    "\n",
    "tokenize_file(token_path, xLAN_path, tokenized_path, batch_size=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenizer.detokenizer import detokenize_data\n",
    "\n",
    "tokens = \"6 32 34 76 6 37 35 76 6 24 26 76 6 29 28 76 5 55 49 76 5 62 52 76 5 15 25 76 6 45 44 76 4 23 59 76 4 54 45 76 6 40 41 76 1 46 62 76 2 31 24 76 5 22 37 76 3 7 31 76 6 69 68 76 4 59 66 76 6 35 26 81 4 47 26 81 5 52 35 76 4 66 45 81 2 38 45 81 4 26 40 76 5 35 25 81 2 24 25 81 5 37 52 76 5 49 43 76 5 52 42 76 2 25 17 76 2 45 52 76 1 39 55 76 3 54 38 76 4 40 33 76 5 42 36 76 2 17 24 76 5 36 46 76 3 31 23 76 5 46 29 76 6 56 57 76 4 30 37 76 2 24 27 76 4 37 46 76 2 27 9 76 6 13 12 76 6 48 50 76 6 21 19 76 5 43 58 76 6 19 18 76 5 58 52 79 6 61 52 81 2 9 18 81 3 14 22 76 2 18 32 76 5 29 35 76 6 41 42 76 5 35 18 76 4 33 26 76 6 28 27 76 6 50 51 76 6 27 34 81 6 51 44 81 4 46 19 76 6 44 53 79 1 62 54 76 2 32 68 79 1 54 45 76 2 68 52 79 1 45 54 76 4 26 19 81 3 22 19 81 2 52 68 77 1 54 45 76 3 23 29 77 3 38 37 76 2 53 54 78 71 74\"\n",
    "token_path = \"./src/tokenizer/xlanplus_tokens.json\"\n",
    "\n",
    "print(detokenize_data(tokens, token_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove lines with more than x tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_lines_with_too_many_tokens(\n",
    "    input_file_path, output_file_path, token_limit=500\n",
    "):\n",
    "    with open(input_file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    print(f\"Number of lines in {input_file_path}: {len(lines)}\")\n",
    "    lines_to_keep = []\n",
    "    removed_count = 0\n",
    "\n",
    "    for line in lines:\n",
    "        if len(line.split()) <= token_limit:\n",
    "            lines_to_keep.append(line)\n",
    "        else:\n",
    "            removed_count += 1\n",
    "\n",
    "    print(f\"Number of lines in {output_file_path}: {len(lines_to_keep)}\")\n",
    "    with open(output_file_path, \"w\") as file:\n",
    "        file.writelines(lines_to_keep)\n",
    "\n",
    "    return removed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"D:\\LEON Safe\\Datasets\\lichess_db_standard_rated_2023-09.tok\"\n",
    "output_file_path = \"D:\\LEON Safe\\Datasets\\lichess_db_standard_rated_2023-09_max_500.tok\"\n",
    "removed_lines = remove_lines_with_too_many_tokens(input_file_path, output_file_path)\n",
    "print(f\"Number of removed lines: {removed_lines}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create files with only one line for each starting sequence\n",
    "\n",
    "### Break down big Dataset to smaller dataset with more variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Remove Duplicate Lines by Start Sequence\n",
    "    -------------------\n",
    "\n",
    "    Removes duplicate lines from a file by comparing the first n tokens of each line.\n",
    "    The first n tokens are called the start sequence.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    lines: The lines to remove duplicates from.\n",
    "    start_sequences: A list of integers. Each integer is the length of the start sequence.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    A dictionary with the start sequence length as key and the list of lines as value.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def remove_duplicates_by_start_sequence(lines, start_sequences, debug=True):\n",
    "    result = {length: [] for length in start_sequences}\n",
    "    starting_sequences_sets = {length: set() for length in start_sequences}\n",
    "\n",
    "    for line in lines:\n",
    "        tokens = line.strip().split()\n",
    "        for length in start_sequences:\n",
    "            sequence = \" \".join(tokens[:length])\n",
    "            if sequence not in starting_sequences_sets[length]:\n",
    "                starting_sequences_sets[length].add(sequence)\n",
    "                result[length].append(line)\n",
    "\n",
    "    if debug:\n",
    "        print(\"Original:\", len(lines))\n",
    "        for length in start_sequences:\n",
    "            print(\n",
    "                f\"Stripped duplicates for first {length} tokens:\", len(result[length])\n",
    "            )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:\\LEON Safe\\Datasets\\lichess_db_standard_rated_2023-09_max_500.tok\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "start_sequences = [13, 16, 20, 24]\n",
    "results = remove_duplicates_by_start_sequence(lines, start_sequences, debug=True)\n",
    "\n",
    "for length, saved_lines in results.items():\n",
    "    out_path = f\"D:\\LEON Safe\\Datasets\\lichess_db_standard_rated_2023-09_max_500_{length}tokens.tok\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.writelines(saved_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert file from xLan to xLan+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.notation_converter import xlan_sequence_to_xlanplus\n",
    "\n",
    "\n",
    "def convert_xlan_to_xlanplus(xlan_file, xlanplus_file):\n",
    "    with open(xlan_file, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    with open(xlanplus_file, \"w\") as file:\n",
    "        for line in lines:\n",
    "            # if empty line or line starts with # copy it\n",
    "            if not line.strip() or line.startswith(\"#\"):\n",
    "                file.write(line)\n",
    "                continue\n",
    "\n",
    "            xlan_plus = xlan_sequence_to_xlanplus(line)\n",
    "            file.write(xlan_plus + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlan_file = \"./data/validation/board_state/board_state_positions.lan\"\n",
    "xlanplus_file = \"./data/validation/board_state/board_state_positions.xlanplus\"\n",
    "convert_xlan_to_xlanplus(xlan_file, xlanplus_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
