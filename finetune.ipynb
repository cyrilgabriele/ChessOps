{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train a Chess LLM on xLAN Datasets/Validate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "from src.train import ChessTrainer\n",
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n",
        "\n",
        "- use datasets in ./data/training to train your model\n",
        "- use DownloadUpload.ipynb to Download a dataset from Hugging Face\n",
        "- use DataPreProcessing.ipynb to create your own dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CUDA\n",
        "Check if CUDA is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Leon-Chess-350k-Plus_LoRA_kasparov_5E_0.0001LR on data/tokens/kasparov_max_768_bos.tok with batch size = 16 for 5 epochs\n",
            "Saving model to models/Leon-Chess-350k-Plus_LoRA_kasparov_5E_0.0001LR\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "dataset = \"data/tokens/kasparov_max_768_bos.tok\"\n",
        "\n",
        "## HYPERPARAMETERS\n",
        "BATCH_SIZE = 16  # use the largest batch size that fits on your GPU\n",
        "SAVE_STEPS = 2000  # how often to save a checkpoint\n",
        "LOGGING_STEPS = 50  # how often to validate model and publish it to Weights & Biases\n",
        "EPOCHS = 5  # how many epochs to train for - how many times to go through the dataset\n",
        "LEARNING_RATE = 0.0001  # learning rate - how fast the model should learn\n",
        "SKIP_VALIDATION = True  # skip validation and only save model checkpoints\n",
        "WEIGHTS_AND_BIASES_ENABLED = True  # enable logging to Weights & Biases\n",
        "USE_FP16 = True  # enable mixed precision training (GPU only)\n",
        "XLANPLUS_ENABLED = True  # use xLanPlus tokenizer\n",
        "\n",
        "PEFT_BASE_MODEL = \"Leon-LLM/Leon-Chess-350k-Plus\" # base model to be loaded (from hugging face) for fine-tuning\n",
        "\n",
        "## CONFIG FOR FINE-TUNING\n",
        "R = 128\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.1\n",
        "\n",
        "peft_config = LoraConfig(  # https://huggingface.co/docs/peft/v0.10.0/en/package_reference/lora#peft.LoraConfig\n",
        "    task_type=\"CAUSAL_LM\", # This does not need to be changed for our use case\n",
        "    inference_mode=False, # don't change this for training, only later for inference\n",
        "    r=R,  # lower means faster training, but might underfit because of less complexity (experiments don't show that training time increases, which is rather weird)\n",
        "    lora_alpha=LORA_ALPHA,  # scaling factor that adjusts the magnitude of the combined result (balances the pretrained model’s knowledge and the new task-specific adaptation)\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    # use_rslora=True, # might work better (not tried yet)\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(AutoModelForCausalLM.from_pretrained(PEFT_BASE_MODEL), peft_config)\n",
        "\n",
        "## MODEL NAME\n",
        "output_dir = \"models/\"\n",
        "model_name = f\"{PEFT_BASE_MODEL.split('/')[1]}_LoRA_kasparov_{EPOCHS}E_{LEARNING_RATE}LR\"\n",
        "\n",
        "## SAVING MODEL\n",
        "output_dir = f\"{output_dir}{model_name}\"\n",
        "\n",
        "print(f\"Training {model_name} on {dataset} with batch size = {BATCH_SIZE} for {EPOCHS} epochs\")\n",
        "print(f\"Saving model to {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/teamspace/studios/this_studio/ChessOps\n",
            "['.git', '.gitignore', 'README.md', 'add_bos.py', 'backup', 'checkDuplicates.py', 'create_tokens.py', 'data', 'dataPreProcessing.ipynb', 'env.yaml', 'finetune.ipynb', 'finetune.py', 'inference.ipynb', 'output', 'pgn_to_xlan.py', 'pipeline.ipynb', 'pipeline.png', 'pipeline.yaml', 'remove_lines.py', 'requirements.txt', 'src', 'statistics', 'streamlit', 'wandb', 'models']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(os.getcwd())\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mschmila7\u001b[0m (\u001b[33mleon-llm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/teamspace/studios/this_studio/ChessOps/wandb/run-20240515_205510-h1j8ufrc</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/leon-llm/ChessOps/runs/h1j8ufrc' target=\"_blank\">models</a></strong> to <a href='https://wandb.ai/leon-llm/ChessOps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/leon-llm/ChessOps' target=\"_blank\">https://wandb.ai/leon-llm/ChessOps</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/leon-llm/ChessOps/runs/h1j8ufrc' target=\"_blank\">https://wandb.ai/leon-llm/ChessOps/runs/h1j8ufrc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='665' max='665' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [665/665 06:20, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.331300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.139600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.905600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.763700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.670600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.572100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.407500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.260400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.186100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.159500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.132700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.123400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.114600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0422af50314d46709df76f04b8d77281",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▂▃▃▄▄▅▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▃▄▄▅▆▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>▇▁▅▁▄▂▆▆▇▆▇█▇</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▆▅▄▄▃▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▄▃▃▃▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>4383443753994240.0</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>665</td></tr><tr><td>train/grad_norm</td><td>0.31985</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.1146</td></tr><tr><td>train_loss</td><td>1.58668</td></tr><tr><td>train_runtime</td><td>381.3096</td></tr><tr><td>train_samples_per_second</td><td>27.825</td></tr><tr><td>train_steps_per_second</td><td>1.744</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">models</strong> at: <a href='https://wandb.ai/leon-llm/ChessOps/runs/h1j8ufrc' target=\"_blank\">https://wandb.ai/leon-llm/ChessOps/runs/h1j8ufrc</a><br/> View project at: <a href='https://wandb.ai/leon-llm/ChessOps' target=\"_blank\">https://wandb.ai/leon-llm/ChessOps</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240515_205510-h1j8ufrc/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = ChessTrainer(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    epochs=EPOCHS,\n",
        "    input_file=dataset,\n",
        "    output_dir=output_dir,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    skip_validation=SKIP_VALIDATION,\n",
        "    weight_and_biases=WEIGHTS_AND_BIASES_ENABLED,\n",
        "    use_FP16=USE_FP16,\n",
        "    notation=\"xLANplus\" if XLANPLUS_ENABLED else \"xLAN\",\n",
        "    peft=peft_model,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Push Model to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Leon-Chess-350k-Plus_LoRA_kasparov_5E_0.0001LR'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check model_name\n",
        "\n",
        "model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d75b116831d144e6a23118b18bc7fef4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/18.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/larscarl/Leon-Chess-350k-Plus_LoRA_kasparov_5E_0.0001LR/commit/b09751d5b499ec45e698d36b11c59bb7d832bb66', commit_message='Upload model', commit_description='', oid='b09751d5b499ec45e698d36b11c59bb7d832bb66', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()\n",
        "peft_model.push_to_hub(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Model from Disk (fine-tuned LoRA model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://huggingface.co/docs/transformers/main/en/peft\n",
        "model_dir = \"./Leon-LLM-Models/V45_GPT2_19k_20E_xLANplus/checkpoint-24000\"\n",
        "peft_model_id = \"./Leon-LLM-Models/V55_V45_GPT2_19k_20E_xLANplus_19k_1E_r128/V55_V45_GPT2_19k_20E_xLANplus_19k_1E_r128\"\n",
        "loaded_model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "loaded_model.load_adapter(peft_model_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.generate_prediction import generate_prediction\n",
        "\n",
        "loaded_model.inference_mode = True\n",
        "loaded_model.eval()\n",
        "input = \"Pd2d4 Pd7d5 Pc2c4 Pc7c6\"\n",
        "loaded_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generate_prediction(input=input, num_tokens_to_generate=3, model=loaded_model, token_path=\"./src/tokenizer/xlan_tokens.json\")[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference (for fine-tuned LoRA model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load model from hugging face\n",
        "\n",
        "from src.generate_prediction import generate_prediction\n",
        "\n",
        "peft_model = get_peft_model(AutoModelForCausalLM.from_pretrained(PEFT_BASE_MODEL), peft_config)\n",
        "peft_model.inference_mode = True\n",
        "peft_model.eval()\n",
        "input = \"Pd2d4 Pd7d5 Pc2c4 Pc7c6\"\n",
        "peft_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generate_prediction(input=input, num_tokens_to_generate=3, model=peft_model, token_path=\"./src/tokenizer/xlan_tokens.json\")[0]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
