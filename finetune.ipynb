{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train a Chess LLM on xLAN Datasets/Validate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "from src.train import ChessTrainer\n",
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n",
        "\n",
        "- use datasets in ./data/training to train your model\n",
        "- use DownloadUpload.ipynb to Download a dataset from Hugging Face\n",
        "- use DataPreProcessing.ipynb to create your own dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CUDA\n",
        "Check if CUDA is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = \"data/tokens/carlsen_max_768.tok\"\n",
        "\n",
        "## HYPERPARAMETERS\n",
        "BATCH_SIZE = 16  # use the largest batch size that fits on your GPU\n",
        "SAVE_STEPS = 2000  # how often to save a checkpoint\n",
        "LOGGING_STEPS = 50  # how often to validate model and publish it to Weights & Biases\n",
        "EPOCHS = 1  # how many epochs to train for - how many times to go through the dataset\n",
        "LEARNING_RATE = 0.0001  # learning rate - how fast the model should learn\n",
        "SKIP_VALIDATION = True  # skip validation and only save model checkpoints\n",
        "WEIGHTS_AND_BIASES_ENABLED = True  # enable logging to Weights & Biases\n",
        "USE_FP16 = True  # enable mixed precision training (GPU only)\n",
        "XLANPLUS_ENABLED = True  # use xLanPlus tokenizer\n",
        "\n",
        "PEFT_BASE_MODEL = \"Leon-LLM/Leon-Chess-350k-BOS\" # base model to be loaded (from hugging face) for fine-tuning\n",
        "# PEFT_BASE_MODEL = \"./Leon-LLM-Models/V45_GPT2_19k_20E_xLANplus/checkpoint-24000\" # base model to be loaded (from disk) for fine-tuning\n",
        "\n",
        "## CONFIG FOR FINE-TUNING\n",
        "R = 128 # 8~26min, 16~26min, 32~26min\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.1\n",
        "\n",
        "peft_config = LoraConfig(  # https://huggingface.co/docs/peft/v0.10.0/en/package_reference/lora#peft.LoraConfig\n",
        "    task_type=\"CAUSAL_LM\", # This does not need to be changed for our use case\n",
        "    inference_mode=False, # don't change this for training, only later for inference\n",
        "    r=R,  # lower means faster training, but might underfit because of less complexity (experiments don't show that training time increases, which is rather weird)\n",
        "    lora_alpha=LORA_ALPHA,  # scaling factor that adjusts the magnitude of the combined result (balances the pretrained modelâ€™s knowledge and the new task-specific adaptation)\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    # use_rslora=True, # might work better (not tried yet)\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(AutoModelForCausalLM.from_pretrained(PEFT_BASE_MODEL), peft_config)\n",
        "\n",
        "## MODEL NAME\n",
        "model_name = f\"TODO/set/model/name\"\n",
        "\n",
        "## SAVING MODEL\n",
        "output_dir = f\"TODO/set/output/dir/{model_name}\"\n",
        "\n",
        "print(f\"Training {model_name} on {dataset} with batch size = {BATCH_SIZE} for {EPOCHS} epochs\")\n",
        "print(f\"Saving model to {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = ChessTrainer(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    epochs=EPOCHS,\n",
        "    input_file=dataset,\n",
        "    output_dir=output_dir,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    skip_validation=SKIP_VALIDATION,\n",
        "    weight_and_biases=WEIGHTS_AND_BIASES_ENABLED,\n",
        "    use_FP16=USE_FP16,\n",
        "    notation=\"xLANplus\" if XLANPLUS_ENABLED else \"xLAN\",\n",
        "    peft=peft_model,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Push Model to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()\n",
        "# model.push_to_hub(\"your-name/bigscience/mt0-large-lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Model from Disk (fine-tuned LoRA model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://huggingface.co/docs/transformers/main/en/peft\n",
        "model_dir = \"./Leon-LLM-Models/V45_GPT2_19k_20E_xLANplus/checkpoint-24000\"\n",
        "peft_model_id = \"./Leon-LLM-Models/V55_V45_GPT2_19k_20E_xLANplus_19k_1E_r128/V55_V45_GPT2_19k_20E_xLANplus_19k_1E_r128\"\n",
        "loaded_model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "loaded_model.load_adapter(peft_model_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.generate_prediction import generate_prediction\n",
        "\n",
        "loaded_model.inference_mode = True\n",
        "loaded_model.eval()\n",
        "input = \"Pd2d4 Pd7d5 Pc2c4 Pc7c6\"\n",
        "loaded_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generate_prediction(input=input, num_tokens_to_generate=3, model=loaded_model, token_path=\"./src/tokenizer/xlan_tokens.json\")[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference (for fine-tuned LoRA model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load model from hugging face\n",
        "\n",
        "from src.generate_prediction import generate_prediction\n",
        "\n",
        "peft_model = get_peft_model(AutoModelForCausalLM.from_pretrained(PEFT_BASE_MODEL), peft_config)\n",
        "peft_model.inference_mode = True\n",
        "peft_model.eval()\n",
        "input = \"Pd2d4 Pd7d5 Pc2c4 Pc7c6\"\n",
        "peft_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generate_prediction(input=input, num_tokens_to_generate=3, model=peft_model, token_path=\"./src/tokenizer/xlan_tokens.json\")[0]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
