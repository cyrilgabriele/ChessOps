{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532bff60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T09:43:40.436556Z",
     "iopub.status.busy": "2024-05-08T09:43:40.436402Z",
     "iopub.status.idle": "2024-05-08T09:43:40.440474Z",
     "shell.execute_reply": "2024-05-08T09:43:40.440144Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.007448,
     "end_time": "2024-05-08T09:43:40.441508",
     "exception": false,
     "start_time": "2024-05-08T09:43:40.434060",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# declare a list tasks whose products you want to use as inputs\n",
    "upstream = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c2deb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T09:43:40.444466Z",
     "iopub.status.busy": "2024-05-08T09:43:40.444351Z",
     "iopub.status.idle": "2024-05-08T09:43:40.446058Z",
     "shell.execute_reply": "2024-05-08T09:43:40.445800Z"
    },
    "papermill": {
     "duration": 0.004028,
     "end_time": "2024-05-08T09:43:40.446886",
     "exception": false,
     "start_time": "2024-05-08T09:43:40.442858",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "product = {\"nb\": \"/Users/larscarlschmid/Documents/_repos/ChessOps/output/finetune.ipynb\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c88bc74d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T09:43:40.449837Z",
     "iopub.status.busy": "2024-05-08T09:43:40.449700Z",
     "iopub.status.idle": "2024-05-08T09:43:42.597602Z",
     "shell.execute_reply": "2024-05-08T09:43:42.597260Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 2.150679,
     "end_time": "2024-05-08T09:43:42.598724",
     "exception": false,
     "start_time": "2024-05-08T09:43:40.448045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from src.train import ChessTrainer\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576067d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T09:43:42.601493Z",
     "iopub.status.busy": "2024-05-08T09:43:42.601302Z",
     "iopub.status.idle": "2024-05-08T09:43:42.603427Z",
     "shell.execute_reply": "2024-05-08T09:43:42.603175Z"
    },
    "papermill": {
     "duration": 0.004344,
     "end_time": "2024-05-08T09:43:42.604262",
     "exception": false,
     "start_time": "2024-05-08T09:43:42.599918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## HYPERPARAMETERS\n",
    "BATCH_SIZE = 16  # use the largest batch size that fits on your GPU\n",
    "SAVE_STEPS = 2000  # how often to save a checkpoint\n",
    "LOGGING_STEPS = 50  # how often to validate model and publish it to Weights & Biases\n",
    "EPOCHS = 1  # how many epochs to train for - how many times to go through the dataset\n",
    "LEARNING_RATE = 0.0001  # learning rate - how fast the model should learn\n",
    "SKIP_VALIDATION = True  # skip validation and only save model checkpoints\n",
    "WEIGHTS_AND_BIASES_ENABLED = False  # enable logging to Weights & Biases\n",
    "USE_FP16 = True  # enable mixed precision training (GPU only)\n",
    "XLANPLUS_ENABLED = True  # use xLanPlus tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9e9814",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T09:43:42.606862Z",
     "iopub.status.busy": "2024-05-08T09:43:42.606745Z",
     "iopub.status.idle": "2024-05-08T09:43:42.608598Z",
     "shell.execute_reply": "2024-05-08T09:43:42.608306Z"
    },
    "papermill": {
     "duration": 0.004267,
     "end_time": "2024-05-08T09:43:42.609539",
     "exception": false,
     "start_time": "2024-05-08T09:43:42.605272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## MODEL\n",
    "PEFT_BASE_MODEL = \"Leon-LLM/Leon-Chess-350k-BOS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b07a34ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T09:43:42.612291Z",
     "iopub.status.busy": "2024-05-08T09:43:42.612180Z",
     "iopub.status.idle": "2024-05-08T09:43:42.613941Z",
     "shell.execute_reply": "2024-05-08T09:43:42.613707Z"
    },
    "papermill": {
     "duration": 0.004058,
     "end_time": "2024-05-08T09:43:42.614800",
     "exception": false,
     "start_time": "2024-05-08T09:43:42.610742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## CONFIG FOR FINE-TUNING\n",
    "R = 128  # lower means faster training, but might underfit because of less complexity (experiments don't show that training time increases, which is rather weird)\n",
    "LORA_ALPHA = 32  # scaling factor that adjusts the magnitude of the combined result (balances the pretrained modelâ€™s knowledge and the new task-specific adaptation)\n",
    "LORA_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32c75a6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T09:43:42.617847Z",
     "iopub.status.busy": "2024-05-08T09:43:42.617719Z",
     "iopub.status.idle": "2024-05-08T09:43:42.619527Z",
     "shell.execute_reply": "2024-05-08T09:43:42.619287Z"
    },
    "papermill": {
     "duration": 0.004266,
     "end_time": "2024-05-08T09:43:42.620330",
     "exception": false,
     "start_time": "2024-05-08T09:43:42.616064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## PATHS\n",
    "# dataset = \"/Users/cyrilgabriele/Documents/School/00_Courses/03_MLOPS/04_Project/ChessOps/data/tokens/carlsen_max_768.tok\"\n",
    "dataset = \"./data/tokens/carlsen_max_768.tok\"\n",
    "# output_dir = f\"/Users/cyrilgabriele/Documents/School/00_Courses/03_MLOPS/04_Project/ChessOps/models/\"\n",
    "output_dir = f\"models/\"\n",
    "model_name = f\"{PEFT_BASE_MODEL.split('/')[1]}_LoRA_Carlsen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb4f359d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T09:43:42.622880Z",
     "iopub.status.busy": "2024-05-08T09:43:42.622784Z",
     "iopub.status.idle": "2024-05-08T09:43:42.624615Z",
     "shell.execute_reply": "2024-05-08T09:43:42.624406Z"
    },
    "papermill": {
     "duration": 0.004008,
     "end_time": "2024-05-08T09:43:42.625426",
     "exception": false,
     "start_time": "2024-05-08T09:43:42.621418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    peft_config = LoraConfig(  # https://huggingface.co/docs/peft/v0.10.0/en/package_reference/lora#peft.LoraConfig\n",
    "        task_type=\"CAUSAL_LM\",  # This does not need to be changed for our use case\n",
    "        inference_mode=False,  # don't change this for training, only later for inference\n",
    "        r=R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "    )\n",
    "\n",
    "    peft_model = get_peft_model(\n",
    "        AutoModelForCausalLM.from_pretrained(PEFT_BASE_MODEL), peft_config\n",
    "    )\n",
    "\n",
    "    return peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51795619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T09:43:42.627866Z",
     "iopub.status.busy": "2024-05-08T09:43:42.627769Z",
     "iopub.status.idle": "2024-05-08T09:43:42.629958Z",
     "shell.execute_reply": "2024-05-08T09:43:42.629738Z"
    },
    "papermill": {
     "duration": 0.004352,
     "end_time": "2024-05-08T09:43:42.630793",
     "exception": false,
     "start_time": "2024-05-08T09:43:42.626441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataset, output_dir, debug=True):\n",
    "    if debug:\n",
    "        print(f\"model: {model}\")\n",
    "        print(f\"dataset: {dataset}\")\n",
    "        print(f\"output_dir: {output_dir}\")\n",
    "\n",
    "    trainer = ChessTrainer(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        epochs=EPOCHS,\n",
    "        input_file=dataset,\n",
    "        output_dir=output_dir,\n",
    "        save_steps=SAVE_STEPS,\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        skip_validation=SKIP_VALIDATION,\n",
    "        weight_and_biases=WEIGHTS_AND_BIASES_ENABLED,\n",
    "        use_FP16=USE_FP16,\n",
    "        notation=\"xLANplus\" if XLANPLUS_ENABLED else \"xLAN\",\n",
    "        peft=model,\n",
    "    )\n",
    "\n",
    "    # trainer.train() # TODO: uncomment later\n",
    "    print(\"trainer.train()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60208d2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T09:43:42.633534Z",
     "iopub.status.busy": "2024-05-08T09:43:42.633436Z",
     "iopub.status.idle": "2024-05-08T09:43:42.635067Z",
     "shell.execute_reply": "2024-05-08T09:43:42.634839Z"
    },
    "papermill": {
     "duration": 0.003773,
     "end_time": "2024-05-08T09:43:42.635805",
     "exception": false,
     "start_time": "2024-05-08T09:43:42.632032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def push_model_to_hf(model, name):\n",
    "    # TODO: handle login...\n",
    "    # model.push_to_hub(\"your-name/bigscience/mt0-large-lora\")\n",
    "    print(f\"push_model_to_hf(model={model}, name={name})\")\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54849324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T09:43:42.638298Z",
     "iopub.status.busy": "2024-05-08T09:43:42.638211Z",
     "iopub.status.idle": "2024-05-08T09:43:44.241762Z",
     "shell.execute_reply": "2024-05-08T09:43:44.241490Z"
    },
    "papermill": {
     "duration": 1.605855,
     "end_time": "2024-05-08T09:43:44.242734",
     "exception": false,
     "start_time": "2024-05-08T09:43:42.636879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(76, 768)\n",
      "        (wpe): Embedding(512, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Conv1D()\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=128, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=768, out_features=76, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "dataset: ./data/tokens/carlsen_max_768.tok\n",
      "output_dir: models/Leon-Chess-350k-BOS_LoRA_Carlsen\n",
      "trainer.train()\n",
      "push_model_to_hf(model=PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(76, 768)\n",
      "        (wpe): Embedding(512, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Conv1D()\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=128, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=768, out_features=76, bias=False)\n",
      "    )\n",
      "  )\n",
      "), name=Leon-Chess-350k-BOS_LoRA_Carlsen)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larscarlschmid/Documents/_repos/ChessOps/venv/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "train_model(model, dataset, output_dir + model_name)\n",
    "push_model_to_hf(model, model_name)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "duration": 4.990085,
   "end_time": "2024-05-08T09:43:44.863336",
   "exception": null,
   "input_path": "/var/folders/h3/3d89qwl94937xcsw4qq0hh2c0000gn/T/tmpufvageyi.ipynb",
   "output_path": "/Users/larscarlschmid/Documents/_repos/ChessOps/output/finetune.ipynb",
   "parameters": {
    "product": {
     "nb": "/Users/larscarlschmid/Documents/_repos/ChessOps/output/finetune.ipynb"
    }
   },
   "start_time": "2024-05-08T09:43:39.873251"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}