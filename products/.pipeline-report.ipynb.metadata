{"timestamp": 1714224749.104367, "stored_source_code": "upstream = None\n# declare a list tasks whose products you want to use as inputs\nupstream = None\n\n## Convert PGN to extended LAN (xLAN)\nfrom src.data_preprocessing.pgn_to_xlan import pgn_to_xlan\n\ndef pgn_to_xlan():\n    # TODO change to valid paths\n    pgn_path = \"/Users/cyrilgabriele/Documents/School/00_Courses/03_MLOPS/04_Project/ChessOps/data/pgn/Carlsen.pgn\"\n    lan_path = \"/Users/cyrilgabriele/Documents/School/00_Courses/03_MLOPS/04_Project/ChessOps/data/xlan/carlsen.xlanplus\"\n\n    min_number_of_moves_per_game = 0\n    number_of_games_to_write = -1  # -1 for all games\n\n    pgn_to_lan = pgn_to_xlan(\n        pgn_path,\n        lan_path,\n        min_number_of_moves_per_game=min_number_of_moves_per_game,\n        number_of_games_to_write=number_of_games_to_write,\n        generate_all_moves=False,\n        log=False,\n        xLanPlus=True,\n    )\n\n    pgn_to_lan.convert_pgn_parallel()\n## Check Common and Duplicate Lines\n\n\"\"\"\nUse check_duplicates_and_common_lines to check if there are duplicates or common lines in two files.\n\"\"\"\nfrom src.data_preprocessing.check_duplicates_and_common_lines import (\n    check_duplicates_and_common_lines,\n)   \n\ndef check_duplicates():\n    \n    \n    # TODO what is the validation file?\n    training_file = \"/Users/cyrilgabriele/Documents/School/00_Courses/03_MLOPS/04_Project/ChessOps/data/xlan/carlsen.xlanplus\"\n    validation_file = (\n        \"/Users/cyrilgabriele/Documents/School/00_Courses/03_MLOPS/04_Project/ChessOps/data/xlan/carlsen.xlanplus\"\n    )\n\n    check_duplicates_and_common_lines(training_file, validation_file, delete_common=False, delete_duplicates_from_file_1=True, delete_duplicates_from_file_2=False)\n## Tokenize Data\n\nfrom src.tokenizer.tokenizer import tokenize_file\n\ndef tokenize_data():    \n    xLAN_path = \"/Users/cyrilgabriele/Documents/School/00_Courses/03_MLOPS/04_Project/ChessOps/data/xlan/carlsen.xlanplus\" # Inout Path \n    token_path = \"./src/tokenizer/xlanplus_tokens.json\" # keep this, correct like this\n    tokenized_path = \"/Users/cyrilgabriele/Documents/School/00_Courses/03_MLOPS/04_Project/ChessOps/data/tokens/carlsen.tok\" # Output path\n\n    tokenize_file(token_path, xLAN_path, tokenized_path, batch_size=20000) # eventually smaller batch size\n## Remove lines with more than x tokens\n\ndef read_file(input_file_path): \n    with open(input_file_path, \"r\") as file:\n        lines = file.readlines()\n    return lines\ndef write_file(output_file_path, lines_to_keep): \n    with open(output_file_path, \"w\") as file:\n        file.writelines(lines_to_keep)\ndef remove_lines_with_too_many_tokens(input_file_path, output_file_path, token_limit=768):\n    lines = read_file(input_file_path)\n    # print(f\"Number of lines in {input_file_path}: {len(lines)}\")\n    lines_to_keep = []\n    removed_count = 0\n\n    for line in lines:\n        if len(line.split()) <= token_limit:\n            lines_to_keep.append(line)\n        else:\n            removed_count += 1\n\n    print(f\"Number of lines in {output_file_path}: {len(lines_to_keep)}\")\n    write_file(output_file_path, lines_to_keep)\n\n    return removed_count\ndef remove(): \n    input_file_path = \"/Users/cyrilgabriele/Documents/School/00_Courses/03_MLOPS/04_Project/ChessOps/data/tokens/carlsen.tok\"\n    output_file_path = \"/Users/cyrilgabriele/Documents/School/00_Courses/03_MLOPS/04_Project/ChessOps/data/tokens/carlsen_max_768.tok\"\n    removed_lines = remove_lines_with_too_many_tokens(input_file_path, output_file_path)\n    print(f\"Number of removed lines: {removed_lines}\")", "params": {}}