{"timestamp": 1714228293.659593, "stored_source_code": "# declare a list tasks whose products you want to use as inputs\nupstream = None\nupstream = [\"checkDuplicates\"]\ndef tokenize_data():\n    from src.tokenizer.tokenizer import tokenize_file\n    \n    xLAN_path = \"/Users/cyrilgabriele/Documents/School/00_Courses/03_MLOPS/04_Project/ChessOps/data/xlan/carlsen.xlanplus\" # Inout Path \n    token_path = \"./src/tokenizer/xlanplus_tokens.json\" # keep this, correct like this\n    tokenized_path = \"/Users/cyrilgabriele/Documents/School/00_Courses/03_MLOPS/04_Project/ChessOps/data/tokens/carlsen.tok\" # Output path\n\n    tokenize_file(token_path, xLAN_path, tokenized_path, batch_size=20000) # eventually smaller batch size\ntokenize_data()", "params": {}}